Contexte

Datalo.co est un organisme de formation, et permet à ses professionnels d’intervenir chez
divers clients et structures pour développer les compétences de leurs salariés.
En plus de cette activité, Datalo.co est aussi une entreprise de conseil dans le domaine de
la donnée. L’entreprise propose différents services liés au cycle de vie de la donnée, allant
de la collecte et du stockage à des solutions d’intelligence artificielle, en passant par de
l’analyse de données.
C’est de cette dernière problématique que découle le besoin du projet présenté plus bas,
puisque c’est un projet d’analyse de données qui demanda la conception et la création d’une
base de données, ainsi que sa mise à disposition.
1. Client
Le client est Wampark, un ensemble de bases de loisir présentes sur un total de 8 sites
dispersés en France. Ces bases de loisir proposent diverses activités aquatiques et
estivales, telles que le ski nautique, le kneeboard ou le wakeski.
De cette description se dégagent deux informations sur les données créées par l’activité de
cette entreprise : Un volume important de données générées de manière quotidienne et une
saisonnalité dans le flux des données
2. Problématique
2.1. Compréhension du besoin
Pour ce projet, le client cherche à développer des outils de tableau de bords pour la
visualisation de ses données afin de mieux piloter ses opérations.
La réalisation de ces tableaux de bord nécessite la création d’une base de données
structurée au préalable, portant sur les données générées par l’entreprise au cours des
années passées.

2.2. Données
Le client n’a pas la main complète sur la gestion de ses données, puisqu’il sous-traite à une
autre structure. Pour avoir accès à ses données, il utilise une clé d’accès avec laquelle il
peut requêter une API.
De plus, pour les besoins de l’analyse, nous tenons à ajouter des données géographiques
pour nous permettre une analyse du chiffre d'affaires par région ou département.

4

Nous allons donc utiliser deux sources de données : des données extraites d’une API au
format Json et des données issues de data.gouv au format CSV

Extraction

1. Concept
L’extraction des données nécessaire est découpée en deux étapes :
-L’extraction initiale : C’est une étape ponctuelle, lancée sur commande une seule fois et qui
va récupérer toutes les données collectées jusqu’à présent. On va donc récupérer les
données de data.gouv, et toutes les données possibles via l’API.
-l’extraction quotidienne : C’est une étape automatisée qui va récupérer uniquement les
données acquises dans la journée afin de maintenir la base à jour. Ici, on ne récupère plus
les données issues de data.gouv
Tous les scripts d’extraction des données Python ont été réalisés en utilisant Amazon Web
Services, et plus particulièrement les fonctionnalités de Lambda Functions et de Step
Functions, qui permettent l’automatisation et l’imbrication de code. Ces scripts sont
notamment versionnés sur un dépôt distant sur Github.
2. Extraction initiale

Pour l’extraction initiale, on va utiliser la clé d’accès du client pour récupérer les données de
toutes les tables via l’API. Pour ce faire, un script Python itère sur une liste contenant le nom
de toutes les tables nécessaires.
Dans le même temps, on récupère les données issues de data.gouv au format CSV. Ces
données présentent la liste des communes de France ainsi que les codes postaux,
départements et régions associés.
Ces données sont ensuite enregistrées au format Json dans un Bucket S3 sur AWS, qui
permet le stockage de données non structurées.
Pour chaque table, on stocke dans un fichier Json l’horodatage de la collecte. Cette étape
ne présente aucun intérêt pour l’analyse, mais est indispensable pour l’étape suivante de
l’extraction.

5

3. Extraction quotidienne
Les scripts d’extractions quotidiennes fonctionnent de manière quasi-similaires aux scripts
initiaux, à quelques exceptions près :
● Plus besoin d’extraire les données issues de data.gouv
● On utilise un filtre au moment de la requête API pour récupérer les données créées
ou mises à jour passées une certaine date
● On récupère pour cela le dernier horodatage pour chaque table
● On stocke ensuite un nouvel horodatage
Ces scripts sont automatisés à l’aide de tâches Cron, qui permet de les exécuter
automatiquement tous les soirs, quelques heures après la fermeture des sites.
Ceci permet de collecter les données de manière quotidienne.
Création de base de données

1. Modélisation
Pour la création de la base de données, nous avons utilisé Azure Database. La base de
données relationnelles est la suivante :

Modèle Physique de Données

6

2. Ingestion
L’ingestion des données est automatique et se déclenche au moment de la création d’un
objet dans le Bucket S3. La création d’un fichier CSV ou JSON va déclencher les scripts
python responsables de l’import de ce fichier, puis sa transformation via SQLAlchemy en
donnée compréhensible pour la base de données Azure.
Toutes les colonnes vont être transformées au bon format SQL définis au moment de la
création de la table et dans le MPD. Les données sont ensuite envoyées dans la base, à un
rythme différent pour chaque table.
En effet, si un trop grand volume de données est envoyé pendant une certaine période, la
base de données peut planter. Pour éviter cela, les scripts d’ingestion n’envoient pas plus de
1 MB par seconde. Cela correspond à un nombre de lignes par seconde qui est évidemment
différent pour chaque table.
3. Données personnelles
Comme vous avez pu l’observer dans le modèle de données montré plus tôt, la base de
données présente une table “Client” qui contient des données personnelles, qui tombent
sous le cadre du RGPD. Il convient donc d’affecter un traitement particulier à ces données
pour être conforme à la législation européenne.
La méthode employée dans ce projet est la pseudonymisation, qui nous permet d’encrypter
les données personnelles avec une clé de chiffrement :
Dans la base, les données personnelles sont transformées de sorte à devenir inutilisable
sans la clé de chiffrement, qui est placée dans un coffre-fort virtuel. Ainsi, si quelqu’un
parvient à avoir accès à notre base de données, il ne pourra pas exploiter les données
personnelles des clients, qui seront donc protégées.

7

Mise à disposition
1. API REST
Pour mettre la base de données à la disposition des équipes, nous avons utilisé FastAPI,
pour la création d’un total de 16 points de terminaison permettant d’obtenir autant de
marqueurs statistiques et d’indicateurs de performances clés importants pour la
compréhension des données du client.

Exemples des points de terminaisons présents dans la documentation de l’API

Ces indicateurs de performances sont obtenus au moyen de requêtes SQL, faisant
intervenir parfois des jointures entre diverses tables, ainsi que des fonctions d'agrégations
pour obtenir des indicateurs statistiques.
2. Exemple de point de terminaison

Requête SQL derrière le point de terminaison “Top_clients”

8

Conclusion
Au long de ce rapport, j’ai pu expliciter les différentes étapes mises en œuvre lors de la
création d’une base de données ayant pour but d’être exploitée pour de l’analyse de
données.
En réponse à la problématique de base, la création d’une base de données structurées a
permis plus tard dans l’avancement du projet la mise en place de tableaux de bords
permettant au client de piloter efficacement son activité
Au travers de ce projet, j’ai pu mettre en place et développer des compétences de Data
Engineering, mais aussi m’exercer à la tâche complexe qu’est la transcription du besoin
client, qui est d’autant plus complexe lorsqu’on travaille sur une partie aussi émergente d’un
projet que l’est la conception et la création d’une base de données.
meosm pp 
smd 
 
 ae r arhvn p p 
 qeqr mm accès 
 ar 
 r  dhjf accèse 
 z
  z
   
    portant  
    sdf
      z
      e 
      qqzse 

       








vacances CSVz sa ez
 qqzse quelques sd se b   
 quotidienne sa s  schema s entre net_et_veri_donnee 
test 


